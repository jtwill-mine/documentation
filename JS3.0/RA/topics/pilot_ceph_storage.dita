<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="pilot_ceph_storage">
  <title>Pilot Bundle with Ceph Storage</title>
  <shortdesc>The Pilot bundle includes three (3) storage nodes. You can set up a Ceph cluster on these nodes, which can be tied into Cinder, Glance, and Nova. With Ceph you will need to plan out the different services.</shortdesc>
  <body>
    <p>See Table 4 and Table 5 for Hardware Configurations. The Pilot Bundle with Ceph Storage nodes include:</p>
    <ul>
      <li>Node 1: R620 Solution Admin Host with the Red Hat OpenStack Manager Installed</li>
      <li>Nodes 2 - 4: R620 OpenStack Controllers</li>
      <li>Nodes 5-10 R620 (pictured, or optionally R720) Nova Compute Nodes</li>
      <li>Nodes 11 â€“ 13: R720xd Storage Nodes; adds three (3)  Storage Nodes to the bundle</li>
      <li>Network Switches: Two (2) Force10 S4810, and one (1) Force10 S55</li>
    </ul>
    <fig>
      <title>Pilot Bundle with Ceph Storage</title>
      <image href="../../Customization/OpenTopic/common/artwork/pilot_bundle_ceph.png"></image>
    </fig>
    <p>The Ceph cluster provides data protection through replication, block device cloning, and snapshots. By default, the data is striped across the cluster. The number of storage nodes in a single cluster can scale to hundreds of nodes and many petabytes in size. Ceph considers the physical placement of storage nodes when deciding how data is replicated. By defining fault domains, like a rack, row, and data center, and defining the Ceph position of the nodes and disks, Ceph using the position in the decision for data replication, thereby reducing the probability that a given failure results is the loss of more than one data replica.</p>
    <p>There are two services in the Ceph storage cluster, the OSD and the MON (monitor). The OSD services data to the Ceph clients from disks on the storage nodes. Generally, there is one OSD process per disk drive. All OSDs run on the storage nodes. The MON process is used by the Ceph clients and other Ceph processes to determine the composition of the cluster and where data is located. There should be a minimum of three of these MON processes for the Ceph cluster. The MON processes are installed on all OpenStack controller nodes.</p>
    <p>If the load of the clients querying the cluster causes the MON processes to take an inordinate amount of processing, then additional MON processes can be added to the cluster using dedicated machines, then retiring the MON processes co-located on either the Storage Node or OpenStack Controller nodes.</p>
    <p>The Storage Network vLAN is the same network described in the Ceph documentation at the public network. The Storage Cluster Network vLAN is the same network described in the Ceph documentation as the cluster network.</p>
    <p>A special distribution of Ceph is used in this solution: Ceph Enterprise v1.2. That distribution uses the Firefly release of Ceph (v 0.81). Ceph Enterprise also includes the Calamari Ceph cluster management client. Ceph Enterprise is installed on a virtual machine that runs on the Solution Admin Host. The Solution Admin Host also includes Ceph troubleshooting and servicing tools and utilities.</p>
    <p>The Solution Admin Host must have access to the controller, compute and storage nodes through the Private API Access vLAN in order to manage Ceph and for the monitoring process on the storage node to return status and performance telemetry.</p>
    <p>The controller nodes must have access to the storage nodes through the Storage Network vLAN in order for the Ceph clients on the controller nodes to be able to query the Ceph MON processes for the state and configuration of the cluster.</p>
    <p>The compute nodes must have access to the storage nodes through the Storage Network vLAN in order for the Ceph client on that node to interact with the storage nodes and the OSDs and the Ceph MON processes.</p>
    <p>The storage nodes must have access to the Storage Network as previously stated and to the Storage Cluster Network vLAN.</p>
    <fig>
      <title>Pilot Bundle with Ceph Storage</title>
      <image href="../../Customization/OpenTopic/common/artwork/pilot_ceph_logical_network.png" align="left" ></image>
    </fig>
  </body>
</topic>

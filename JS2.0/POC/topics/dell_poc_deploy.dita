<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN"
"topic.dtd">
<topic id="dell_pilot_deploy">
  <title>Dell PoC Deployment Guide</title>

  <body>
    <p/>

    <section><title>Configure Foreman Server</title><p>If the provisioning
    interface on the provisioned host is not the first interface returned in
    the puppet data, Foreman will overwrite the configuration in the database.
    This can cause issues later. To prevent Foreman from changing the IP and
    MAC information to reflect the puppet data, change the
    <b>ignore_puppet_facts_for_provisioning</b> setting<ol>
        <li>Log into the Foreman user interface.</li>

        <li>Move the mouse over the Administer drop down menu on the right
        side of the interface.</li>

        <li>Select Settings from the menu.</li>

        <li>Select the Provisioning tab.</li>

        <li>Select Value for the<b> ignore_puppet_facts_for_provisioning</b>
        setting. <ul>
            <li>Change this setting to true.</li>
          </ul></li>
      </ol></p></section>

    <section><title>Hammer Command Preparation</title>During the steps in this
    document, the <b>hammer</b> command is used to more easily perform the
    configurations. Usage of the hammer command requires identifiers for the
    various pieces of information stored within the Foreman server. This
    includes identifiers for the defined installation media, partitions,
    templates, hosts, as well as other items</section>

    <p>A checklist file, dell-pilot-checklist.pdf, is available to easily keep
    track of this information. Download and print the Dell Pilot Checklist
    from <b><xref format="html"
    href="https://wiki.opencrowbar.org/pages/viewpage.action?pageId=6326050">https://wiki.opencrowbar.org</xref></b>.</p>

    <p>Write the various information gathered and needed on the checklist for
    reference as needed.</p>

    <p>Several steps in this document use files to configure the environment.
    These files are available in a TAR file called dell-pilot-deploy.tgz. This
    TAR file is available from <xref format="html"
    href="https://wiki.opencrowbar.org/pages/viewpage.action?pageId=6326050"><b>https://wiki.opencrowbar.org</b></xref>.</p>

    <p>Download the tarfile and unzip it into the /root/pilot directory.</p>

    <codeblock># mkdir /root/pilot
# cd /root/pilot
# tar xzvf /PATH/TO/FILE/dell-pilot-deploy.tgz</codeblock>

    <section><lines/><title>The Hammer Command</title><ol>
        <li>Install the packages that contain the <b>hammer</b>
        command<codeblock># yum -y install "*hammer*"
</codeblock>This should install the following packages: <msgblock>rubygem-hammer_cli-doc-0.1.1-12.el6sat.noarch
rubygem-hammer_cli-0.1.1-12.el6sat.noarch
rubygem-hammer_cli_foreman-0.1.1-16.el6sat.noarch
rubygem-hammer_cli_foreman-doc-0.1.1-16.el6sat.noarch
</msgblock></li>

        <li>Configure the <b>hammer</b> command to display <b>200</b> items
        per page. This just makes it easier to read everything. <ul>
            <li>Replace the value for the <b>:per_page:</b> option in the
            /<b>etc/hammer/cli_config.yml</b> file.</li>
          </ul></li>

        <li>Configure the <b>hammer</b> command to not prompt for a
        username/password when connecting.<ul>
            <li>Add the following to the beginning of the
            <b>/etc/hammer/cli_config.yml </b>file. Replace the name and
            password appropriately<msgblock>:foreman:
 :username: 'admin'
 :password: 'changeme'
</msgblock></li>
          </ul></li>
      </ol><p/><title>Hammer help</title>The <b>hammer</b> command takes the
    <b>--help</b> option. This option can be used with most of its subcommands
    as well. It is useful to see the various options that can be
    used.<p/></section>

    <section><title>Configure the Installation Medium</title><p>Configure the
    installation medium that will be used to provision the hosts.</p><p>Use
    the <b>hammer medium create</b> command to add the entry.</p><p>The
    command requires the --name, --os-family and --path options.</p><p>The
    <b>name</b> option should specify a name that is appropriate for the
    installation.</p><p>The <b>os-family</b> should remain <b>Redhat</b> in
    most cases.</p><p>The <b>path</b> option specifies the path or URL to the
    installation tree</p></section>

    <section><p/><title>Satellite Server </title></section>

    <p>The path included in the example below should work with most satellite
    installation, just replace <b>SATELLITE_SERVER</b> with the appropriate<b>
    FQDN</b>.</p>

    <msgblock>hammer medium create --name "Dell OSP Pilot" --os-family Redhat \
 --path 'http://SATELLITE_SERVER/ks/dist/ks-rhel-$arch-server-$major-$version'</msgblock>

    <section><lines/><title>Local ISO on Foreman Node </title></section>

    <ol>
      <li>mkdir /usr/share/foreman/public/iso</li>

      <li>Copy the RHEL7 iso to the /root directory of the foreman node</li>

      <li>Edit /etc/fstab</li>

      <li>Add the following line to the end: <msgblock>/root/RHEL-7.0-Server-x86_64-dvd.iso /usr/share/foreman/public/iso iso9660 loop,ro 0 0</msgblock></li>

      <li>mount –a<msgblock>hammer medium create --name "Dell OSP Pilot" --os-family Redhat \
 --path 'http://FOREMAN_SERVER/iso'
</msgblock></li>
    </ol>

    <section><lines/><title>Local ISO on Solution Admin Host</title><ol>
        <li>On the Solution Admin Host:<ol outputclass="lower-alpha">
            <li>yum install httpd</li>

            <li>Configure httpd with any options you want. Defaults will work,
            but are not secure.</li>

            <li>copy the ISO to the SAH in /store/data/iso</li>

            <li>"mount -o loop ISO NAME /mnt"</li>

            <li>"mkdir /store/data/iso/RHEL6.5" (Name of the OS)</li>

            <li>"rsync -av /mnt/ /store/data/iso/RHEL6.5"</li>

            <li>"umount /mnt"</li>

            <li>"ln -s /store/data/iso/RHEL6.5 RHEL6.5"</li>
          </ol></li>

        <li>Validate the web interface works<msgblock>http://ipaddress of foreman/RHEL6.5/README</msgblock></li>
      </ol></section>

    <section><p/><title>Note the Medium ID</title></section>

    <p>After the medium is created, execute the <b>hammer medium list</b>
    command.</p>

    <p>Take note of the <b>ID</b> (first column) for the newly created medium.
    This will be needed later</p>

    <section><lines/><title>Configure the Partition Table </title></section>

    <p>Configure the partition table that the provisioned hosts will use.
    These partition table is provided in the following file.</p>

    <p>dell-poc-controller.partition Partition table for the POC controller
    nodes.</p>

    <p>dell-poc-compute.partition Partition table for the POC compute
    nodes</p>

    <p>Use the hammer partition-table create command to install the partition
    tables. Specify an appropriate NAME for each partition table and specify
    the file that contains the data. Install only the needed partition
    tables</p>

    <msgblock># # hammer partition-table create --name dell-poc-controller --os-family Redhat \
 --file /root/poc/dell-poc-controller.partition
# hammer partition-table create --name dell-poc-compute --os-family Redhat \
 --file /root/poc/dell-poc-compute.partition
</msgblock>

    <p/>

    <p>After the partition table is created, execute the <b>hammer
    partition-table list</b> command.</p>

    <p>Take note of the ID of the newly created partition table.</p>

    <section><lines/><title>Configure the Operating System </title></section>

    <p>A operating system definition for Red Hat Enterprise Linux 6.5 was
    created when the Foreman server registered with itself. But a definition
    for Red Hat Enterprise Linux 7.0 was not.</p>

    <p>Create a new operating system definition for Red Hat Enterprise Linux
    7.0.</p>

    <p>Use the <b>hammer os create</b> command to create the definition.</p>

    <msgblock># hammer os create --name "RedHat" --major 7 --minor 0 --family Redhat </msgblock>

    <p/>

    <p>After creating the operating system, execute the <b>hammer os list</b>
    command.</p>

    <p>Take note of the ID’s for the new RedHat 6.5 and RedHat 7.0 operating
    systems.</p>

    <p>Associate the x86_64 architecture with the RedHat 6.5 and RedHat 7.0
    operating systems.</p>

    <p>Execute the <b>hammer os add-architecture</b> command for each
    operating systems <b>ID</b>.</p>

    <codeblock>hammer os add-architecture --architecture x86_64 --id RHEL6.5_OS_ID
hammer os add-architecture --architecture x86_64 --id RHEL7.0_OS_ID</codeblock>

    <p/>

    <p>The appropriate partition table needs to be associated with the
    operating system.</p>

    <p>Use the <b>hammer os add-ptable</b> command to associate the partition
    table to the operating system Id.</p>

    <p>This command must be executed four times, once for each combination of
    partition table and the RedHat 6.5 and RedHat 7.0 operating system
    IDs.</p>

    <codeblock>hammer os add-ptable --ptable-id P_ID --id OS_ID</codeblock>

    <section><lines/><title>Configure Subnets </title></section>

    <p>A subnet called OpenStack was created automatically during
    installation. Execute<b> hammer subnet lis</b>t and note the ID of this
    subnet.</p>

    <msgblock># hammer subnet list </msgblock>

    <p/>

    <p>IP addresses can be automatically assigned to newly provisioned hosts
    from this subnet. To do this, a range of IPs to assign must be
    defined.</p>

    <p>Use the <b>hammer subnet update</b> command to assign the range of IP
    addresses and to also set the default gateway for the provisioned
    hosts.</p>

    <p>If the foreman server is acting as the gateway, the gateway address is
    the IP address of the Foreman servers provisioning interface.</p>

    <msgblock># hammer subnet update --id SN_ID --from START_IP_RANGE --to END_IP_RANGE \ --gateway GATEWAY_IP </msgblock>

    <section><lines/><title>Configure Templates </title></section>

    <p>Four template files are provided and used to provision hosts.</p>

    <simpletable relcolwidth="249* 751*">
      <strow>
        <stentry>dell-osp-ks.template</stentry>

        <stentry>Provisioning template that provides the kickstart
        file.</stentry>
      </strow>

      <strow>
        <stentry>dell-osp-pxe.template</stentry>

        <stentry>PXE template that contains the PXE configuration.</stentry>
      </strow>

      <strow>
        <stentry>interface_config.template</stentry>

        <stentry>Snippet that provides a means to configure extra interfaces
        during installation. This template is called from the kickstart
        template.</stentry>
      </strow>

      <strow>
        <stentry>bonding_snippet.template</stentry>

        <stentry>Snippet that provides a means to bond interfaces during
        installation. This template is called from the kickstart
        template.</stentry>
      </strow>
    </simpletable>

    <p>Create the templates using the <b>hammer template create</b>
    command.</p>

    <msgblock># hammer template create --name "Dell OpenStack Kickstart Template" --type provision \
 --operatingsystem-ids ""OS_ID_RH6.5, OOS_ID_RH7.0" --file /root/pilot/dell-osp-ks.template
# hammer template create --name "Dell OpenStack PXE Template" --type PXELinux \
 --operatingsystem-ids ""OS_ID_RH6.5, OOS_ID_RH7.0" --file /root/pilot/dell-osp-pxe.template
# hammer template create --name "bond_interfaces" --type snippet \
 --file /root/pilot/bonding_snippet.template
# hammer template create --name "interface_config" --type snippet \
 --file /root/pilot/interface_config.template
</msgblock>

    <p>After the templates are created, use the <b>hammer template list</b>
    command to determine the IDs of the templates.</p>

    <p>The provisioning and PXE templates must be associated with the
    operating systems. Use the <b>hammer os update</b> command to update the
    template associations.</p>

    <p>Execute the following command for each of the RedHat 6.5 and RedHat 7.0
    IDs. This command also associates the installation medium to the operating
    system.</p>

    <msgblock># hammer os update --config-template-ids "KS_ID, PXE_ID" \
 --medium-ids MEDIUM_ID --id OS_ID</msgblock>

    <p>The templates are now associated with the operating system, they must
    now be set as the default templates for the operating systems.</p>

    <p>Execute the<b> hammer os set-default</b> template command for the
    provisioning and PXELinux templates. The command will be executed two
    times.</p>

    <msgblock># hammer os set-default-template --config-template-id TMPLT_ID --id OS_ID
</msgblock>

    <p>The <b>hammer os info</b> command can be used to check the
    configuration of the operating systems.</p>

    <msgblock># hammer os info --id 1
Id: 1
Full name: RedHat 6.5
Release name:
Family: Redhat
Name: RedHat
Major version: 6
Minor version: 5
Partition tables:
Default templates:
 Dell OpenStack Kickstart Template (provision)
 Dell OpenStack PXE Template (PXELinux)
Architectures:
Installation media:
 Red Hat Satellite
Templates:
 Dell OpenStack Kickstart Template (provision)
 Dell OpenStack PXE Template (PXELinux)
Parameters:</msgblock>

    <section><lines/><title>Gather More Information </title></section>

    <p>A few more IDs are required in order to install a host. Execute the
    following commands and take note of the <b>appropriate IDs</b>.</p>

    <ul>
      <li>Environments<msgblock># hammer environment list</msgblock></li>

      <li>Domains<msgblock># hammer domain list</msgblock></li>

      <li>Puppet Proxy<msgblock># hammer proxy list</msgblock></li>

      <li>Architectures<msgblock># hammer architecture list</msgblock></li>
    </ul>

    <section><lines/></section>

    <section><title>Configure facts updates</title><p>Foreman updates the host
    information using the Puppet facts. Foreman updates the provisioning
    information with the first interface returned from the Puppet facts. To
    prevent this, perform the following steps. </p></section>

    <ol>
      <li>Log into the Foreman UI.</li>

      <li>Select the <b>Administer</b> drop down on the top right of the
      window.</li>

      <li>Select <b>Settings</b>. Select the <b>Provisioning</b> tab.</li>

      <li>Edit the ignore_puppet_facts_for_provisioning setting and set it to
      true.</li>
    </ol>

    <section><lines/></section>

    <section><title>Provisioning the Nodes</title><p>Provision the nodes using
    the following command. Variables are being used to make it easier to use
    the hammer command. </p><p>Simply set the <b>NAME, PTABLE</b>, and
    <b>MAC</b> variables appropriately for a host, then execute the h<b>ammer
    host create</b> command. Once the host is created, reset the variables for
    the next host and execute the same hammer command again. </p><p>Repeat
    this until all hosts are created. Make sure to change the root password on
    the command line as needed. </p><simpletable relcolwidth="287* 713*">
        <strow>
          <stentry>NAME</stentry>

          <stentry>The hostname of the provisioned host.</stentry>
        </strow>

        <strow>
          <stentry>PTABLE</stentry>

          <stentry>The ID of the partition table that should be used when
          deploying this host. This ID is the same for all the compute nodes,
          but the controller uses a different partition table ID.</stentry>
        </strow>

        <strow>
          <stentry>MAC</stentry>

          <stentry>The hardware address of the provisioning interface for the
          node.</stentry>
        </strow>
      </simpletable><p>Change the remaining IDs in the command as appropriate.
    </p><msgblock># NAME=CHANGEME
# PTABLE=CHANGEME
# MAC=CHANGEME
# hammer host create --name "${NAME}" --root-password 'CHANGEME' \
 --build true --enabled true --managed true --environment-id 2 \
 --domain-id 1 --puppet-proxy-id 1 --operatingsystem-id 2 \
 --subnet-id 1 --architecture-id 1 --medium-id 9 \
 --partition-table-id ${PTABLE} \
 --mac ${MAC}</msgblock><p/></section>

    <section><title>Get host information</title><p>The IDs of the newly
    defined hosts are needed to finish their configuration. </p><p>Execute the
    <b>hammer host list</b> command to get the host IDs. Take note of these<b>
    IDs</b>.</p><codeblock># hammer host list</codeblock><p/></section>

    <section><title>Configure the Operating System for Updates.
    </title><p>Configure the Operating system definition with the parameters
    for registering the provisioned host for updates. The <b>hammer os
    set-parameter</b> command is used to set the parameters. </p></section>

    <p>The following parameters are set.</p>

    <simpletable relcolwidth="256* 744*">
      <strow>
        <stentry>subscription_manager</stentry>

        <stentry>Specifies that the hosts will register with Subscription
        Manager. (true or false)</stentry>
      </strow>

      <strow>
        <stentry>subscription_manager_username</stentry>

        <stentry>The username of the Subscription Manager account to register
        to.</stentry>
      </strow>

      <strow>
        <stentry>subscription_manager_password</stentry>

        <stentry>The password for the Subscription Manager account.</stentry>
      </strow>

      <strow>
        <stentry>subscription_manager_pool</stentry>

        <stentry>The ID of the pool to attach the host to</stentry>
      </strow>

      <strow>
        <stentry>subscription_manager_repos</stentry>

        <stentry>The repositories that should be enabled after the host is
        registered</stentry>
      </strow>
    </simpletable>

    <p/>

    <section><title>Determine Pool ID</title><p>To determine the pool id, you
    must have an existing server that is registered to the RedHat Hosted
    Services. This server must also be registered using the same credentials
    as the ones being used in this environment. </p><p>Once the server is
    correctly registered, execute the subscription-manager list --all
    --available command to see the available subscription pools.</p><p>The
    command will output a list of available pools. Each section of information
    lists what the subscription provides, its pool ID, how many are a
    vailable, the type of system it is for, as well as other information.
    </p><p>Determine the correct pool ID needed for this environment and take
    note of it. Place close attention to the System Type. The System Type can
    be Virtual or Physical. You cannot use a pool marked as Virtual for a
    physical server. </p><msgblock># subscription-manager list --all --available
[OUTPUT ABBREVIATED]
Subscription Name: Red Hat Cloud Infrastructure, Standard (8-sockets)
Provides: Red Hat Beta
 Red Hat OpenStack Beta
 JBoss Enterprise Application Platform
 Red Hat Software Collections (for RHEL Server)
 Red Hat Enterprise Virtualization
 Oracle Java (for RHEL Server)
 Red Hat OpenStack
 Red Hat Enterprise MRG Messaging
 Red Hat Enterprise Linux Server
 Red Hat Enterprise Linux High Availability (for RHEL Server)
 Red Hat Software Collections Beta (for RHEL Server)
 Red Hat Enterprise Linux Load Balancer (for RHEL Server)
 Red Hat CloudForms
SKU: MCT2861
Pool ID: aaaa111bbb222ccc333ddd444eee5556
Available: 7
Suggested: 1
Service Level: Standard
Service Type: L1-L3
Multi-Entitlement: No
Ends: 09/23/2015
System Type: Physical
[OUTPUT ABBREVIATED]</msgblock></section>

    <section><p>These steps should be performed for both the RedHat 6.5 and
    RedHat 7.0 operating systems. Define the OS_ID variable to the ID of the
    operating system then execute the remaining commands. </p><p>Change the
    OS_ID to the other ID and repeat the commands again. .</p><p>Make sure to
    specify the appropriate <b>username, password</b>, and <b>pool ID</b> by
    replacing the <b>CHANGEME's</b> and <b>POOL_ID.</b></p><msgblock># OS_ID=2
# hammer os set-parameter --operatingsystem-id ${OS_ID} \
 --name subscription_manager --value true
# hammer os set-parameter --operatingsystem-id ${OS_ID} \
 --name subscription_manager_username --value CHANGEME
# hammer os set-parameter --operatingsystem-id ${OS_ID} \
 --name subscription_manager_password --value 'CHANGEME'</msgblock><lines/><title>Optional
    Proxy Settings </title><p>Proxy settings for the subscription-manager and
    yum commands can be set by defining the needed parameters. These
    parameters are completely optional if not needed.</p><p>Although defining
    and using the following parameters will work for most environments, they
    may not work for all. Setting the parameters gives the provisioning
    template the information to set the proxy information using the
    subscription-manager config command.</p><p>If these setting do not work in
    your environment, the kickstart file may need to be manually modified.
    This can be done using the Foreman user interface.. </p><p>The following
    parameters can be set using the <b>hammer os set-parameter</b> command as
    above.</p><simpletable relcolwidth="271* 729*">
        <strow>
          <stentry>subscription_manager_proxy</stentry>

          <stentry>The proxy server to use, if needed.</stentry>
        </strow>

        <strow>
          <stentry>subscription_manager_proxy_port</stentry>

          <stentry>The proxy port to use, if needed.</stentry>
        </strow>

        <strow>
          <stentry>subscription_manager_proxy_user</stentry>

          <stentry>The proxy username, if needed.</stentry>
        </strow>

        <strow>
          <stentry>subscription_manager_proxy_password</stentry>

          <stentry>The proxy username password, if needed.</stentry>
        </strow>
      </simpletable></section>

    <section><lines/><title>Controller Node </title></section>

    <p>Set nics parameter on the controller host. This provides the kickstart
    file the needed information to configure the two non-provisioned (Public
    API, Private API) interfaces.</p>

    <p>The value for the parameter consists of a space separated list of
    <b>key=value</b> pairs. Each pair contains the configuration information
    for a single nic. This list is entered in the same format a s a bash
    associative array. The entire value is enclosed within single tickmarks
    and parentheses. <i>( …)</i></p>

    <p>Each key/value is in the following format:</p>

    <ul>
      <li><b>[IFACE]="parameters"</b></li>
    </ul>

    <p>An example of defining two interfaces:</p>

    <msgblock># hammer host set-parameter --host-id 3 --name nics \ 
--value ([em1]="onboot static aa:bb:cc:dd:ee:ff 192.168.0.110/255.255.255.0" [em2]="onboot static 11:22:33:44:55:66
192.169.10.110/255.255.255.0")</msgblock>

    <p>The parameters consist of the following:</p>

    <simpletable relcolwidth="259* 741*">
      <strow>
        <stentry>onboot</stentry>

        <stentry>The interface is enabled when the system boots. Default is
        <i>disabled</i>.</stentry>
      </strow>

      <strow>
        <stentry>dhcp | static | none</stentry>

        <stentry>The interface gets its network configuration using DHCP,· the
        network configuration is staticaly configured, or the interface has no
        network configuration. If no option is specified, <i>dhcp</i> is
        assumed.</stentry>
      </strow>

      <strow>
        <stentry>xx:xx:xx:xx:xx:xx</stentry>

        <stentry>The hardware address of the interface.</stentry>
      </strow>

      <strow>
        <stentry>x.x.x.x/y.y.y.y</stentry>

        <stentry>The IP address and Network mask of the interface. Used only
        when static is specified. Must be in the format presented.</stentry>
      </strow>
    </simpletable>

    <p>Execute the following command to set the nic parameters for the two
    other interfaces used by the controller node. Replace the <b>HOST_ID,
    IFACE, MAC</b>, and<b> IP/MASK </b>parameters as appropriate.</p>

    <msgblock># hammer host set-parameter --host-id HOST_ID --name nics \
 --value '([IFACE]="onboot static MAC IP/MASK" [IFACE]="onboot static MAC IP/MASK")'
</msgblock>

    <p/>

    <section><title>Compute Nodes</title><p>The nics parameter must be set to
    configure the three remaining ( Nova Public, Nova Private, Private API)
    interfaces used on the compute nodes. </p><p>Execute the following hammer
    command on each compute node. Note that the third interface specified is
    the IFACE_NOVA_PRIVATE interface. This interface has no network
    configuration, but should be enabled upon boot. </p><msgblock># hammer host set-parameter --host-id HOST_ID --name nics \
 --value '([IFACE]="onboot static MAC IP/MASK" [IFACE]="onboot static MAC IP/MASK"
[IFACE_NOVA_PRIVATE]="onboot none MAC")'</msgblock></section>

    <section><p/></section>

    <section><title>Start Provisioning</title><p>PXE boot each of the nodes.
    After they are booted, make sure the networks are configured
    appropriately. </p></section>

    <p/>

    <section><title>Assigning Hostgroups</title></section>

    <section><title>Configure Hostgroup Parameters</title><p>The default
    values for the hostgroup parameters are specified in the
    <b>dell-poc.yaml.erb</b> file. This file consists of a couple of sections.
    The top section contains parameters that are most commonly changed. Under
    normal situations, the parameters in the remaining sections should not be
    changed. </p><p>Below are listed the parameters that should be changed for
    each environment. The remaining parameters are noted within the file.
    </p><simpletable relcolwidth="245* 755*">
        <strow>
          <stentry>passwd_auto</stentry>

          <stentry>The password to use in most places.</stentry>
        </strow>

        <strow>
          <stentry>controller_admin_host</stentry>

          <stentry>The IP address of the controller nodes interface that has
          administrative network traffic.</stentry>
        </strow>

        <strow>
          <stentry>controller_priv_host</stentry>

          <stentry>The IP address of the controller nodes interface connected
          to the Private API network.</stentry>
        </strow>

        <strow>
          <stentry>controller_pub_host</stentry>

          <stentry>The IP address of the controller nodes interface connected
          to the Public API network.</stentry>
        </strow>

        <strow>
          <stentry>nova_public_net</stentry>

          <stentry>Network address and mask for the Nova Public
          Network</stentry>
        </strow>

        <strow>
          <stentry>nova_public_iface</stentry>

          <stentry>Compute nodes interface that is connected to the Nova
          Public Network</stentry>
        </strow>

        <strow>
          <stentry>nova_private_net</stentry>

          <stentry>Network address and mask for the Nova Private
          Network</stentry>
        </strow>

        <strow>
          <stentry>nova_private_iface</stentry>

          <stentry>Compute nodes interface that is connected to the Nova
          Private Network</stentry>
        </strow>

        <strow>
          <stentry>private_api_net</stentry>

          <stentry>Network address and mask for the Private API
          Network</stentry>
        </strow>

        <strow>
          <stentry>private_api_iface</stentry>

          <stentry>Compute nodes interface that is connected to the Private
          API Network</stentry>
        </strow>
      </simpletable><p><b>Caution</b> should be taken when changing parameters
    since the parameter type is determined when it is defined in the
    <b>dell-poc.yaml.erb</b> file. </p><p>When changing values, ensure the new
    value is in the same syntax as the previous one. Whether a parameter is
    enclosed in quotes, square brackets, or spans multiple lines determines
    the type of parameter. </p><p>Edit the <b>dell-poc.yaml.erb</b> file and
    make the appropriate changes.</p><p> </p></section>

    <section><title>Apply hHostgroup Parameters</title><p>The
    <b>rubygen-foreman_api</b> package must be installed to apply the changes
    in the d<b>ell-poc.yaml.erb</b> file. </p><codeblock># yum install -y rubygem-foreman_api</codeblock><p>Change
    to the<b> /usr/share/openstack-foreman-installer </b>directory and execute
    the <b>bin/quickstack_defaults.rb</b> command as shown below.
    </p><codeblock># cd /usr/share/openstack-foreman-installer
# bin/quickstack_defaults.rb -g config/hostgroups.yaml -d ~/dell-poc.yaml.erb -v parameters
</codeblock><p>The network_overrides parameter cannot easily be set using the
    <b>dell-poc.yaml.erb</b> file. It is set using the <b>hammer sc-param
    update</b> command. </p><p>First, the parameter <b>ID </b>must be
    determined, then the settings applied. Replace <b>VLAN </b>with the
    starting VLAN number to be used in the environment. Make sure the syntax
    of the line does not change. </p><codeblock># ParamId=$( hammer sc-param list --per-page 1000 --search network_overrides \
 | awk '/network_overrides/ {print $1}')
# hammer sc-param update --id ${ParamId} \
 --default-value "{'vlan_start': VLAN, 'force_dhcp_release': 'false'}" --override yes</codeblock><p/></section>

    <section><title>Configure Nodes </title><p>After the nodes are installed,
    the must have a hostgroup assigned to them. </p><p>The IDs of the
    hostgroups must be determined. Execute the <b>hammer hostgroup list</b>
    command. Take note of the IDs for the Controller (Nova Network) and
    Compute (Nova Network) hostgroups. </p><codeblock># hammer hostgroup list</codeblock><p/></section>

    <section><title>Add Controller hostgroup</title><p>Apply the Controller
    (Nova Network) hostgroup to the controller node using the <b>hammer host
    update </b>command. </p><codeblock># hammer host update --hostgroup-id HOSTGROUP_ID --id HOST_ID</codeblock><p>Once
    the hostgroup is applied, log into the controller node and execute the
    following command to pull the hostgroup configuration. </p><codeblock># puppet agent -t -dv |&amp; tee /root/puppet.out</codeblock><p>This
    command pipes a copy of the output to the /root/puppet.out file for later
    review. Watch the output or review the /root/puppet.out file for errors.
    </p></section>

    <section><title>Add Compute hostgroup</title><p>Add the hostgroups to the
    hosts one at a time. Make sure to run <b>puppet agent -t -dv |&amp; tee
    /root/puppet.out</b> between each. </p><p><b>Do not</b> add the next host
    in the list until the previous one is finished. Failure to do so can lead
    to a race condition that prevents proper installation and configuration of
    the compute nodes.</p><codeblock># hammer host update --hostgroup-id HOSTGROUP_ID --id HOST_ID</codeblock><p>Wait
    for each compute node to finish its configuration before starting the next
    one. </p></section>
  </body>
</topic>
